{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c24564-bdf6-4704-8d46-35f94cec31f3",
   "metadata": {},
   "source": [
    "# What is a Vector Database!?\n",
    "\n",
    "<img src=\"https://redis.io/wp-content/uploads/2024/02/vector-db-101-inline-1-1200x628-1.png?&auto=webp&quality=85,75&width=500\" width=600>\n",
    "\n",
    "Vector databases are special databases meant to store content such as PDFs, Blogs, Word Documents, Images, Audio Files, and even Videos as embedding vectors, allowing for semantic based retrieval! But what does any of that mean?\n",
    "\n",
    " We'll get into it right here, in this notebook we'll look at:\n",
    "1. What is a vector embedding and the rise of different embedding models\n",
    "2. Visualizing embeddings and embedding space\n",
    "3. Calculating distances between embeddings\n",
    "4. Using distances/similarity for information retrieval\n",
    "5. Methods of indexxing vectors\n",
    "6. Full vector database and retrieval augmented generation setup\n",
    "\n",
    "### Beginning with a look at an embedding:\n",
    "\n",
    "In the process of a large language model, embedding is generally the first step to convert discrete tokens (like words or subwords) into dense vector representations, which will allow the rest of the network to do the math necessary to predict the next word. (Embeddings capture much more than just the first step of an LLM, but we'll get into that!)\n",
    "\n",
    "For this we need to somehow convert natural language into a numerical representation that still captures the meaning of the original text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cb74b95-4d43-45b8-a305-a8e2042dd3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /Users/blessinvarkey/miniconda3/lib/python3.11/site-packages (5.23.0)\n",
      "Requirement already satisfied: matplotlib in /Users/blessinvarkey/miniconda3/lib/python3.11/site-packages (3.8.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/blessinvarkey/miniconda3/lib/python3.11/site-packages (1.3.2)\n",
      "Requirement already satisfied: openai in /Users/blessinvarkey/miniconda3/lib/python3.11/site-packages (1.42.0)\n",
      "Requirement already satisfied: pandas in /Users/blessinvarkey/miniconda3/lib/python3.11/site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy in /Users/blessinvarkey/miniconda3/lib/python3.11/site-packages (1.23.5)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install plotly matplotlib scikit-learn openai pandas numpy os\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msk-proj-R7FfMXt2aM7HRtJob0VaXxalMFr8SdZctweEz5OVMORXvc1omr1pLHJ7DDT3BlbkFJ3hls0UMlL6SlsTc26atF5rDBRTPktThxk3zBoIk3nLL1USPyExU759D20A\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "!pip install plotly matplotlib scikit-learn openai pandas numpy os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cf57f1c-0ef2-4800-a192-af85cae308ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Setup\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d33c6a8-2b7a-46f4-8aae-abbea2d48141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "# Set your OpenAI API key directly\n",
    "openai.api_key ='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66ca9d13-8889-4643-9740-45d832482c9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.Embedding, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Your code here\u001b[39;00m\n\u001b[1;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-embedding-ada-002\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m embedding \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Embedding for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHello!\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Embedding, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# Set API key from environment variable\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')  # Ensure this environment variable is set\n",
    "\n",
    "# Define the text\n",
    "text = \"Hello!\"\n",
    "\n",
    "# Request embeddings from OpenAI API\n",
    "response = openai.embeddings.create(\n",
    "    model=\"text-embedding-ada-002\",  # Check the latest model name\n",
    "    input=text\n",
    ")\n",
    "\n",
    "# Extract embedding from the response\n",
    "embedding = response['data'][0]['embedding']\n",
    "\n",
    "# Print the results\n",
    "print(\"--- Embedding for 'Hello!' ---\\n\")\n",
    "print(embedding[:20], \"...\", len(embedding) - 20, \"More Values\")\n",
    "print(\"\\n--- Length ---\\n\")\n",
    "print(len(embedding), \"Dimensions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5f2ad-118f-434e-97f6-f0d8fa172992",
   "metadata": {},
   "source": [
    "---\n",
    "# All Starts With the Vector\n",
    "\n",
    "A **[vector](https://www.pinecone.io/learn/vector-embeddings-for-developers/#What-can-I-do-with-vector-embeddings)** is a mathematical structure with a magnitude (or size) and a direction. For example, we can think of the vector as a point in space, with the “direction” being an arrow from (0,0,0) to that point in the vector space.\n",
    "\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc6a923ebc992a3622134ebd95e9992b269299918-214x216.png&w=640&q=75\" width=200>\n",
    "\n",
    "This is a representation of the vector `[1,1]` in two dimensions\n",
    "\n",
    "<img src=\"https://www.intmath.com/vectors/img/235-3D-vector.png\" width=200>\n",
    "\n",
    "And an example in three dimenions, `[2,3,5]`\n",
    "\n",
    "You might be more familiar with the idea of a vector from physics, which uses vectors to calculate natural things like velocity, momentum, force, electromagnetic fields and weight.\n",
    "\n",
    "<img src=\"https://labs.phys.utk.edu/mbreinig/phys221core/modules/m3/images/Image1361.gif\" width=200>\n",
    "\n",
    "Vectors are an ideal form of data for Machine Learning operations for a few reasons:\n",
    "1. They provide a compact way to represent multi-dimensional data, where each dimension can correspond to a feature or attribute of the data.\n",
    "2. Vector operations (addition, subtraction, dot product) enable efficient computations, and CPUs/GPUs are optimized for this allowing for operations on concepts (e.g king - man + woman = queen)\n",
    "3. Neurons in artificial neural networks process and output vectors and the underlying parameters are essentially a collection of vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3cb3117-ff5b-4de5-9971-d54ceb6e388d",
   "metadata": {},
   "source": [
    "### So how is a concept or idea in text converted to numbers?\n",
    "\n",
    "The simplest embedding of an idea is **one-hot encoding**\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:674/1*9ZuDXoc2ek-GfHE2esty5A.png\" width=450>\n",
    "\n",
    "Each word is represented by a binary vector, where the encoded word/meaning is made 1 and the other values are put to 0. While this is straight forward, there are limitations:\n",
    "1. **Sparsity**: For large vocabularies, most of the vector will be zeros, leading to inefficient storage and computation.\n",
    "2. **No semantic information**: It doesn't capture any meaning or relationship between words.\n",
    "3. **Fixed vocabulary**: It can't handle words outside the predefined vocabulary.\n",
    "\n",
    "To address this, researchers developed more sophisticated embedding methods. One significant step in this evolution was the introduction of co-occurrence matrices.\n",
    "\n",
    "Co-occurence matrix for the sentence: I like deep learning. I like NLP. I enjoy flying.\n",
    "  \n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/37840Screenshot%202021-06-18%20at%2012.19.48%20AM.png\" width=450>\n",
    "\n",
    "This aims to capture the frequency with which words appear together within a certain context window. This method begins to incorporate semantic information by assuming that words appearing in similar contexts have related meanings.\n",
    "\n",
    "This brought some advancements:\n",
    "\n",
    "1. **Captures some semantic information**: Words used in similar contexts will have similar vector representations.\n",
    "2. **Provides richer representations**: Each word is represented by a vector of co-occurrence counts rather than a single 1 in a sparse vector.\n",
    "3. **Can handle out-of-vocabulary words**: New words can be represented based on their co-occurrences with known words.\n",
    "\n",
    "But still some limitations:\n",
    "\n",
    "1. **High dimensionality**: The size of the matrix grows quadratically with the vocabulary size.\n",
    "2. **Sparsity**: Many word pairs may never co-occur, leading to many zero entries.\n",
    "3. **Lack of fine-grained semantic relationships**: While it captures some context, it doesn't fully represent complex language patterns.\n",
    "\n",
    "The latest advancements in embedding technology leverage sophisticated machine learning techniques to create rich, context-aware representations of words and concepts. These models address many limitations of earlier methods and offer powerful capabilities for various NLP tasks.\n",
    "\n",
    "<img src=\"https://tinkerd.net/img/tensorflow/bert-embedding-layer.png\" width=450>\n",
    "\n",
    "Unlike static embeddings that we've seen before, these models can generate different representations for the same word based on its context in a sentence. This allows us to capture rich contextual understanding, such as distinguishing between different meanings of the same word based on context.\n",
    "\n",
    "Recent embedding models take advantage of scaled deep learning models like GPTs or BERT that are trained on massive amounts of text data for next word/token/sentence prediction tasks. Through this training process they learn to represent (or almost learn!) words as high dimensional vectors. These learned representations are what's used as the embedding!\n",
    "\n",
    "In a way, you can almost take a task specific language model, remove the last task specific layer, and its suddenly an embedding model itself. Embeddings then are the neural networks learned understanding of the input text, transformed over and over as it flows through the net.\n",
    "\n",
    "While many embedding models are derived from large language models, there are also models specifically trained for the task of creating embeddings, both for text and other modalities. These models are often optimized for particular use cases or domains.\n",
    "\n",
    "<img src=\"https://images.ctfassets.net/kftzwdyauwt9/fbc4f633-9ad4-4dc2-3809c22df5e0/0bd2d5abf90d052731538613e4a42668/overview-a.svg\" width=450>\n",
    "\n",
    "Examples include:\n",
    "- **[Instructor-XL](https://huggingface.co/hkunlp/instructor-xl)**: Instruction-finetuned text embedding model that can generate embeddings tailored to specific tasks and domains without further finetuning.\n",
    "- **[BGE-3](https://huggingface.co/BAAI/bge-m3)**: Multilingual embedding model trained for cross-lingual information retrieval, document similarity analysis and multilingual text classification\n",
    "- **[CLIP](https://huggingface.co/openai/clip-vit-large-patch14)**: Image focused embedding model trained to perform zero-shot image classification which enables cross-modal retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87406ca-3926-4e2f-85b2-1182efa608ca",
   "metadata": {},
   "source": [
    "### What Can We Do With That?\n",
    "\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Ff3b48ac3b18dec0f97fbcf6eb2a294a2993eb4b1-214x216.png&w=640&q=75\" width=200>\n",
    "\n",
    "Embeddings flowing through something like a language model are phenomenal for the LLMs calculations and predictions, but we can also take advantage of this meaningful dense vector representation. Specifically, comparing these vectors together can give us a measure of semantic similarity between words, sentences, or even entire documents.\n",
    "\n",
    "Let's visualize and apply these embeddings now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554fc02a-0cf2-4231-9b9b-69583a661e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/alucek/Documents/Jupyter_Notebooks/embeddings-guide/100_embeddings.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ac56c-be50-460b-a53f-1d4ad2d20705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word, df):\n",
    "    \"\"\"Retrieve the embedding for a given word.\"\"\"\n",
    "    try:\n",
    "        return eval(df[df['Word'] == word]['embedding'].iloc[0])\n",
    "    except IndexError:\n",
    "        raise ValueError(f\"Word '{word}' not found in the embedding dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cdcacf-a40a-4fdc-b5d8-6c88fc20f16a",
   "metadata": {},
   "source": [
    "### 1D Representation\n",
    "\n",
    "We mere mortals live and understand only 3 dimensions, so visualizing an embedding that is 1536 dimensions can be a little hard! Let's look at some ways to reduce down our embeddings to only a few dimensions to try and capture the meaning on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0fddb0-9123-4400-8858-d8b1a91e6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embedding(embedding, title=\"Embedding Visualization\"):\n",
    "    # Reshape the embedding to a 2D array\n",
    "    embedding_2d = np.array(embedding).reshape(1, -1)\n",
    "    \n",
    "    # Create a diverging colormap centered at zero\n",
    "    cmap = plt.cm.RdYlBu_r\n",
    "    vmax = max(abs(np.min(embedding_2d)), abs(np.max(embedding_2d)))\n",
    "    norm = TwoSlopeNorm(vmin=-vmax, vcenter=0, vmax=vmax)\n",
    "    \n",
    "    # Create the heatmap\n",
    "    fig, ax = plt.subplots(figsize=(20, 3))\n",
    "    im = ax.imshow(embedding_2d, aspect='auto', cmap=cmap, norm=norm)\n",
    "    \n",
    "    # Set title and labels\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Embedding Dimensions\")\n",
    "    plt.ylabel(\"Embedding\")\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax, extend='both')\n",
    "    cbar.set_label('Value', rotation=270, labelpad=15)\n",
    "    \n",
    "    # Remove y-axis ticks\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # Show every 200th tick on x-axis\n",
    "    ax.set_xticks(range(0, len(embedding), 200))\n",
    "    ax.set_xticklabels(range(0, len(embedding), 200))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "visualize_embedding(get_embedding(\"dog\", df), \"Embedding Visualization for dog\")\n",
    "visualize_embedding(get_embedding(\"mango\", df), \"Embedding Visualization for mango\")\n",
    "visualize_embedding(get_embedding(\"pilot\", df), \"Embedding Visualization for pilot\")\n",
    "visualize_embedding(get_embedding(\"breezy\", df), \"Embedding Visualization for breezy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28714fe-069c-45b3-a68d-ca3220f879da",
   "metadata": {},
   "source": [
    "This visualization shows the value of all 1536 dimensions in the word embedding as a color scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3670209e-6a62-4a23-a647-43fd2ae81104",
   "metadata": {},
   "source": [
    "### 2D Representation\n",
    "\n",
    "With 1536-dimensional embeddings, it's impossible to directly visualize the data, so introducing the t-distributed Stochastic Neighbor Embedding model to help! t-SNE is a machine learning algorithm used for dimensionality reduction that will help us reduce this to 2D or 3D, allowing us to plot and observe patterns, clusters, or relationships in our embedding space. It is designed to preserve the relationships between data points while reducing dimensionality. \n",
    "\n",
    "While we won't capture all the nuanced details of our high dimension space, we can see some general trends!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e127c3db-c883-422b-968d-0b18617d24dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string representations of lists to numpy arrays\n",
    "matrix = np.array(df['embedding'].apply(eval).tolist())\n",
    "\n",
    "# Create a t-SNE model and transform the data\n",
    "tsne = TSNE(n_components=2, perplexity=15, random_state=2, init='random', learning_rate=200)\n",
    "vis_dims = tsne.fit_transform(matrix)\n",
    "\n",
    "# Define colors for each category\n",
    "category_colors = {\n",
    "    'Animal': 'red',\n",
    "    'Food': 'green',\n",
    "    'Occupation': 'blue',\n",
    "    'Weather': 'purple'\n",
    "}\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(20, 16))  # Increased figure size for better visibility\n",
    "for category, color in category_colors.items():\n",
    "    category_mask = df['Category'] == category\n",
    "    category_data = vis_dims[category_mask]\n",
    "    words = df['Word'][category_mask]\n",
    "    \n",
    "    plt.scatter(category_data[:, 0], category_data[:, 1], c=color, label=category, alpha=0.6)\n",
    "    \n",
    "    # Add labels to each point\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, (category_data[i, 0], category_data[i, 1]), fontsize=16, alpha=0.7)\n",
    "\n",
    "plt.title(\"Word Embeddings Visualized by Category using t-SNE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d6476e-bb5d-4d34-93b6-edd336e322de",
   "metadata": {},
   "source": [
    "### 3D Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a444491-3685-4dd7-b2be-7052dc551f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string representations of lists to numpy arrays\n",
    "matrix = np.array(df['embedding'].apply(eval).tolist())\n",
    "\n",
    "# Create a t-SNE model and transform the data\n",
    "tsne = TSNE(\n",
    "    n_components=3,\n",
    "    perplexity=10,\n",
    "    n_iter=5000,\n",
    "    learning_rate='auto',\n",
    "    init='pca',\n",
    "    random_state=2\n",
    ")\n",
    "vis_dims = tsne.fit_transform(matrix)\n",
    "\n",
    "category_colors = {\n",
    "    'Animal': 'red',\n",
    "    'Food': 'green',\n",
    "    'Occupation': 'blue',\n",
    "    'Weather': 'purple'\n",
    "}\n",
    "\n",
    "# Create traces for each category\n",
    "traces = []\n",
    "for category, color in category_colors.items():\n",
    "    category_mask = df['Category'] == category\n",
    "    category_data = vis_dims[category_mask]\n",
    "    words = df['Word'][category_mask]\n",
    "    \n",
    "    # Create hover text with only coordinates\n",
    "    hovertext = [f\"X: {x:.2f}, Y: {y:.2f}, Z: {z:.2f}\" \n",
    "                 for x, y, z in category_data]\n",
    "    \n",
    "    trace = go.Scatter3d(\n",
    "        x=category_data[:, 0],\n",
    "        y=category_data[:, 1],\n",
    "        z=category_data[:, 2],\n",
    "        mode='markers+text',\n",
    "        name=category,\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=color,\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        text=words,\n",
    "        textposition=\"top center\",\n",
    "        hovertext=hovertext,\n",
    "        hoverinfo='text',\n",
    "        textfont=dict(size=12)\n",
    "    )\n",
    "    traces.append(trace)\n",
    "\n",
    "# Create the layout\n",
    "layout = go.Layout(\n",
    "    title=\"Word Embeddings Visualized by Category using t-SNE (3D)\",\n",
    "    scene=dict(\n",
    "        xaxis_title='X',\n",
    "        yaxis_title='Y',\n",
    "        zaxis_title='Z',\n",
    "        aspectmode='cube',\n",
    "        camera=dict(\n",
    "            up=dict(x=0, y=0, z=1),\n",
    "            center=dict(x=0, y=0, z=0),\n",
    "            eye=dict(x=1.5, y=1.5, z=1.5)\n",
    "        ),\n",
    "    ),\n",
    "    width=1000,  \n",
    "    height=800,\n",
    "    margin=dict(l=0, r=0, b=0, t=40),\n",
    "    legend=dict(\n",
    "        x=0.9,\n",
    "        y=0.9,\n",
    "        traceorder=\"normal\",\n",
    "        font=dict(size=12),\n",
    "        bgcolor=\"rgba(255, 255, 255, 0.5)\"\n",
    "    ),\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Create the figure and display\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "init_notebook_mode(connected=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0581a484-7c83-4386-99b7-29b9c694e9f4",
   "metadata": {},
   "source": [
    "*Note: This 3D graph doesn't render on github*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a40f42-d2ef-42a0-bfb7-641cf11e5a91",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Calculating Similarity\n",
    "\n",
    "<img src=\"https://images.contentstack.io/v3/assets/bltac01ee6daa3a1e14/bltbfd3d7e3452e01c7/65de3a8c6e7edb400cb4f753/img_blog_image2_inline.png?width=1120&disable=upscale&auto=webp\" width=600>\n",
    "\n",
    "Comparing vector embeddings and determining their similarity is crucial for semantic retrieval. The choice of similarity metric can significantly impact the accuracy of your search results. A key principle is to align the similarity metric of your index with the one used in training your embedding model. For instance, if you're using the [all-MiniLM-L6-v2 model](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2#fine-tuning), which was trained using cosine similarity, employing the same metric in your index will yield the most accurate results.\n",
    "\n",
    "The concept of \"closeness\" between vectors is central to understanding semantic relationships. In the context of word embeddings, two vectors may be considered \"close\" if they represent words used in similar contexts or related to similar ideas. This notion of similarity forms the foundation for the following calculations and their applications in semantic search systems.\n",
    "\n",
    "Here are a couple popular methods for dense vector NLP settings:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bc9485-c9ed-4e45-93f7-31d41ab1cca0",
   "metadata": {},
   "source": [
    "### Euclidean/L2 Distance\n",
    "\n",
    "Measures the straight-line distance between two vectors in a vector space. It ranges from 0 to infinity, where 0 represents identical vectors, and larger values represent increasingly dissimilar vectors.\n",
    "\n",
    "   $\\text{Euclidean distance} = \\|\\mathbf{A} - \\mathbf{B}\\| = \\sqrt{\\sum_{i=1}^n (A_i - B_i)^2}$\n",
    "\n",
    "   Where $\\mathbf{A}$ and $\\mathbf{B}$ are vectors, and $A_i$ and $B_i$ are their respective components.\n",
    "\n",
    "**Euclidean Distance in two Dimensions**\n",
    "\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Ffc175963bced347c4984d95d021cbe423d6154db-416x429.png&w=1080&q=75\" width=300>\n",
    "\n",
    "Commonly applied to embeddings when the magnitude of the vectors is meaningful. It measures the straight-line distance between two embedding vectors in the high-dimensional space. In the context of embeddings, a smaller Euclidean distance indicates greater similarity, making it useful for tasks like finding nearest neighbors or clustering similar items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebd554dd-49c7-43eb-9c2a-a0e3865b79ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hot & Cold:  0.9440164141889588\n",
      "Hot & Programmer:  1.2950602664127295\n"
     ]
    }
   ],
   "source": [
    "def euclidean_distance(vec1, vec2):\n",
    "    \"\"\"Calculate Euclidean distance between two vectors.\"\"\"\n",
    "    return math.sqrt(sum((a - b) ** 2 for a, b in zip(vec1, vec2)))\n",
    "\n",
    "print(\"Hot & Cold: \", euclidean_distance(get_embedding(\"hot\", df), get_embedding(\"cold\", df)))\n",
    "print(\"Hot & Programmer: \", euclidean_distance(get_embedding(\"hot\", df), get_embedding(\"programmer\", df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af55bd7a-b9e6-43af-8ab9-1767d5b50a48",
   "metadata": {},
   "source": [
    "### Dot Product:\n",
    "\n",
    "Measures the product of the magnitudes of two vectors and the cosine of the angle between them. It ranges from -∞ to ∞, where a positive value represents vectors that point in the same direction, 0 represents orthogonal vectors, and a negative value represents vectors that point in opposite directions.\n",
    "\n",
    "$\\mathbf{A} \\cdot \\mathbf{B} = \\sum_{i=1}^n A_i B_i = A_1B_1 + A_2B_2 + \\cdots + A_nB_n$\n",
    "\n",
    "Where $\\mathbf{A}$ and $\\mathbf{B}$ are vectors, and $A_i$ and $B_i$ are their respective components.\n",
    "\n",
    "**Dot Product in Two Dimensions**\n",
    "\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fb2af046e6a6d5d2057f2328171d0bdeba0489a90-338x357.png&w=750&q=75\" width=300>\n",
    "\n",
    "Less commonly used alone for comparing embeddings, the dot product is a fundamental operation in many similarity calculations. For embeddings, it provides a measure of how aligned two vectors are, with higher values indicating greater similarity. It's particularly useful when working with normalized embeddings, as it then effectively captures the cosine similarity. (As seen with OpenAI embeddings, which are normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61d72580-91b1-409c-8a84-1455262de7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hot & Cold:  0.5544164660964531\n",
      "Hot & Programmer:  0.16140940629592512\n"
     ]
    }
   ],
   "source": [
    "def dot_product(vec1, vec2):\n",
    "    \"\"\"Calculate dot product of two vectors.\"\"\"\n",
    "    return sum(a * b for a, b in zip(vec1, vec2))\n",
    "\n",
    "print(\"Hot & Cold: \", dot_product(get_embedding(\"hot\", df), get_embedding(\"cold\", df)))\n",
    "print(\"Hot & Programmer: \", dot_product(get_embedding(\"hot\", df), get_embedding(\"programmer\", df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb89ae3f-73ff-49c0-84f7-0b3451ca822d",
   "metadata": {},
   "source": [
    "### Cosine Similarity:\n",
    "\n",
    "Measures the cosine of the angle between two vectors in a vector space. It ranges from -1 to 1, where 1 represents identical vectors, 0 represents orthogonal vectors, and -1 represents vectors that are diametrically opposed.\n",
    "\n",
    "   $\\text{cosine similarity} = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} = \\frac{\\sum_{i=1}^n A_i B_i}{\\sqrt{\\sum_{i=1}^n A_i^2} \\sqrt{\\sum_{i=1}^n B_i^2}}$\n",
    "\n",
    "   Where $\\mathbf{A}$ and $\\mathbf{B}$ are vectors, and $A_i$ and $B_i$ are their respective components.\n",
    "\n",
    "**Cosine Similarity in Two Dimensions**\n",
    "\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F5a5ba7e0971f7b6dc4697732fa8adc59a46b6d8d-338x357.png&w=750&q=75\" width=300>\n",
    "\n",
    "Widely used for comparing embeddings, especially in natural language processing and information retrieval. It measures the cosine of the angle between two embedding vectors, focusing on their orientation rather than magnitude. This makes it particularly useful for comparing embeddings of different lengths or when the relative direction of vectors is more important than their absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3b21691-6131-4ecc-b5fd-20bfd6ed5317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hot & Cold:  0.5544164875936521\n",
      "Hot & Programmer:  0.16140941386337487\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    dot_prod = sum(a * b for a, b in zip(vec1, vec2))\n",
    "    mag1 = math.sqrt(sum(a ** 2 for a in vec1))\n",
    "    mag2 = math.sqrt(sum(b ** 2 for b in vec2))\n",
    "    return dot_prod / (mag1 * mag2) if mag1 * mag2 != 0 else 0\n",
    "\n",
    "print(\"Hot & Cold: \", cosine_similarity(get_embedding(\"hot\", df), get_embedding(\"cold\", df)))\n",
    "print(\"Hot & Programmer: \", cosine_similarity(get_embedding(\"hot\", df), get_embedding(\"programmer\", df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6536c41-258a-469a-ae87-02feaf832146",
   "metadata": {},
   "source": [
    "### Comparing All At Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1373b541-b1df-4d47-923f-643850d39404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison between 'hot' and 'cold':\n",
      "Euclidean Distance: 0.9440\n",
      "Cosine Similarity: 0.5544\n",
      "Dot Product: 0.5544\n",
      "\n",
      "Comparison between 'hot' and 'warm':\n",
      "Euclidean Distance: 0.8565\n",
      "Cosine Similarity: 0.6332\n",
      "Dot Product: 0.6332\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compare_words(word1, word2, df):\n",
    "    \"\"\"Compare two words using their embeddings.\"\"\"\n",
    "    try:\n",
    "        embedding1 = get_embedding(word1, df)\n",
    "        embedding2 = get_embedding(word2, df)\n",
    "        \n",
    "        euc_dist = euclidean_distance(embedding1, embedding2)\n",
    "        cos_sim = cosine_similarity(embedding1, embedding2)\n",
    "        dot_prod = dot_product(embedding1, embedding2)\n",
    "        \n",
    "        print(f\"Comparison between '{word1}' and '{word2}':\")\n",
    "        print(f\"Euclidean Distance: {euc_dist:.4f}\")\n",
    "        print(f\"Cosine Similarity: {cos_sim:.4f}\")\n",
    "        print(f\"Dot Product: {dot_prod:.4f}\")\n",
    "        print()\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "compare_words(\"hot\", \"cold\", df)\n",
    "compare_words(\"hot\", \"warm\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49ed73-8b57-47c6-9da1-ebd27fbc3900",
   "metadata": {},
   "source": [
    "---\n",
    "# Semantic Retrieval\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/SemanticSearch.png\" width=350>\n",
    "\n",
    "This leads us to one of the most powerful applications - semantic search and retrieval.\n",
    "\n",
    "Instead of the traditional key-query based database, we can use these similarity calculation methods to retrieve *similar* entries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46866947-f207-4303-818c-fe68828edae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Category</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dog</td>\n",
       "      <td>Animal</td>\n",
       "      <td>[0.05113774910569191, -0.01870863139629364, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cat</td>\n",
       "      <td>Animal</td>\n",
       "      <td>[0.025529420003294945, -0.023411665111780167, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>elephant</td>\n",
       "      <td>Animal</td>\n",
       "      <td>[0.0468301959335804, -0.036735061556100845, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lion</td>\n",
       "      <td>Animal</td>\n",
       "      <td>[0.03309255465865135, -0.06823775172233582, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tiger</td>\n",
       "      <td>Animal</td>\n",
       "      <td>[-0.022506916895508766, -0.044839587062597275,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>muggy</td>\n",
       "      <td>Weather</td>\n",
       "      <td>[-0.0029042107053101063, 0.006667866837233305,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>parakeet</td>\n",
       "      <td>Animal</td>\n",
       "      <td>[0.05223187059164047, -0.014343400485813618, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>cabbage</td>\n",
       "      <td>Food</td>\n",
       "      <td>[0.03496464341878891, 0.0027530803345143795, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>therapist</td>\n",
       "      <td>Occupation</td>\n",
       "      <td>[-0.011547889560461044, 0.0051359753124415874,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>drought</td>\n",
       "      <td>Weather</td>\n",
       "      <td>[-0.0014557980466634035, -0.002516623819246888...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word    Category                                          embedding\n",
       "0          dog      Animal  [0.05113774910569191, -0.01870863139629364, -0...\n",
       "1          cat      Animal  [0.025529420003294945, -0.023411665111780167, ...\n",
       "2     elephant      Animal  [0.0468301959335804, -0.036735061556100845, -0...\n",
       "3         lion      Animal  [0.03309255465865135, -0.06823775172233582, 0....\n",
       "4        tiger      Animal  [-0.022506916895508766, -0.044839587062597275,...\n",
       "..         ...         ...                                                ...\n",
       "99       muggy     Weather  [-0.0029042107053101063, 0.006667866837233305,...\n",
       "100   parakeet      Animal  [0.05223187059164047, -0.014343400485813618, -...\n",
       "101    cabbage        Food  [0.03496464341878891, 0.0027530803345143795, 0...\n",
       "102  therapist  Occupation  [-0.011547889560461044, 0.0051359753124415874,...\n",
       "103    drought     Weather  [-0.0014557980466634035, -0.002516623819246888...\n",
       "\n",
       "[104 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('/Users/alucek/Documents/Jupyter_Notebooks/embeddings-guide/new_embeddings.csv')\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "df_combined = pd.concat([df, df2], ignore_index=True)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "403d8de8-fb78-45cf-b451-1ee14b814417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison between 'parakeet' and 'cat':\n",
      "Euclidean Distance: 1.1374\n",
      "Cosine Similarity: 0.3531\n",
      "Dot Product: 0.3531\n",
      "\n",
      "Comparison between 'therapist' and 'mechanic':\n",
      "Euclidean Distance: 1.1238\n",
      "Cosine Similarity: 0.3686\n",
      "Dot Product: 0.3686\n",
      "\n",
      "Comparison between 'cabbage' and 'orange':\n",
      "Euclidean Distance: 1.2075\n",
      "Cosine Similarity: 0.2710\n",
      "Dot Product: 0.2710\n",
      "\n",
      "Comparison between 'drought' and 'thunderstorm':\n",
      "Euclidean Distance: 1.0938\n",
      "Cosine Similarity: 0.4018\n",
      "Dot Product: 0.4018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_words(\"parakeet\", \"cat\", df_combined)\n",
    "compare_words(\"therapist\", \"mechanic\", df_combined)\n",
    "compare_words(\"cabbage\", \"orange\", df_combined)\n",
    "compare_words(\"drought\", \"thunderstorm\", df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "408240c5-7bc4-4d46-a590-8240a8a05914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 similar entries for 'cabbage':\n",
      "Word: cucumber, Category: Food, Similarity: 0.5925\n",
      "Word: lettuce, Category: Food, Similarity: 0.5860\n",
      "Word: carrot, Category: Food, Similarity: 0.5534\n",
      "Word: broccoli, Category: Food, Similarity: 0.5157\n",
      "Word: potato, Category: Food, Similarity: 0.4648\n",
      "-----\n",
      "Top 5 similar entries for 'drought':\n",
      "Word: dry, Category: Weather, Similarity: 0.4736\n",
      "Word: drizzle, Category: Weather, Similarity: 0.4561\n",
      "Word: rainy, Category: Weather, Similarity: 0.4327\n",
      "Word: heatwave, Category: Weather, Similarity: 0.4161\n",
      "Word: thunderstorm, Category: Weather, Similarity: 0.4018\n",
      "-----\n",
      "Top 5 similar entries for 'parakeet':\n",
      "Word: penguin, Category: Animal, Similarity: 0.5124\n",
      "Word: squirrel, Category: Animal, Similarity: 0.4386\n",
      "Word: butterfly, Category: Animal, Similarity: 0.4362\n",
      "Word: kangaroo, Category: Animal, Similarity: 0.4289\n",
      "Word: pear, Category: Food, Similarity: 0.4054\n",
      "-----\n",
      "Top 5 similar entries for 'therapist':\n",
      "Word: doctor, Category: Occupation, Similarity: 0.4646\n",
      "Word: nurse, Category: Occupation, Similarity: 0.4521\n",
      "Word: lawyer, Category: Occupation, Similarity: 0.4417\n",
      "Word: photographer, Category: Occupation, Similarity: 0.4378\n",
      "Word: teacher, Category: Occupation, Similarity: 0.4342\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "def get_top_5_similar(query_word, df_queries, df_combined):\n",
    "    # Get the query embedding\n",
    "    query_embedding = ast.literal_eval(df_queries[df_queries['Word'] == query_word]['embedding'].iloc[0])\n",
    "    \n",
    "    # Convert embeddings to numpy arrays and calculate cosine similarities\n",
    "    similarities = []\n",
    "    for emb_str in df_combined['embedding']:\n",
    "        emb = np.array(ast.literal_eval(emb_str))\n",
    "        query_emb = np.array(query_embedding)\n",
    "        similarity = cosine_similarity(query_emb, emb)\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    # Get indices of top 5 similar entries\n",
    "    top_5_indices = np.argsort(similarities)[-5:][::-1]\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Top 5 similar entries for '{query_word}':\")\n",
    "    for idx in top_5_indices:\n",
    "        word = df_combined.iloc[idx]['Word']\n",
    "        category = df_combined.iloc[idx]['Category']\n",
    "        similarity = similarities[idx]\n",
    "        print(f\"Word: {word}, Category: {category}, Similarity: {similarity:.4f}\")\n",
    "\n",
    "get_top_5_similar(\"cabbage\", df2, df)\n",
    "print(\"-----\")\n",
    "get_top_5_similar(\"drought\", df2, df)\n",
    "print(\"-----\")\n",
    "get_top_5_similar(\"parakeet\", df2, df)\n",
    "print(\"-----\")\n",
    "get_top_5_similar(\"therapist\", df2, df)\n",
    "print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad046a2-762a-411c-9425-c993f181aec4",
   "metadata": {},
   "source": [
    "---\n",
    "# Vector Indexxing & Searching Algorithms\n",
    "\n",
    "<img src=\"https://rockset.com/images/knn/image1.png\" width=400>\n",
    "\n",
    "Obviously its very inneficient and costly to run calculations over everything every time you want to compare a new embedding to your existing embeddings. This is where vector databases come in to set up specific indexxing and querying optimized for similarity comparisons.\n",
    "\n",
    "To do this they use Approximate Nearest Neighbor (ANN) algorithms- efficient techniques for finding similar items in large datasets. ANN algorithms trade perfect accuracy for significantly improved speed, making them ideal for high-dimensional data and large-scale applications. These algorithms use various approaches to quickly narrow down the search space, allowing for fast similarity queries in vector databases.\n",
    "\n",
    "Here are a few ways that Vector DBs approach indexxing embeddings:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844ce4bf-8117-418d-b519-9516898873c5",
   "metadata": {},
   "source": [
    "### K-Dimensional Trees\n",
    "\n",
    "<img src=\"https://groups.csail.mit.edu/graphics/classes/6.838/S98/meetings/m13/kd.gif\" width=600>\n",
    "\n",
    "#### Basic Concept\n",
    "\n",
    "KD-Trees (K-Dimensional Trees) are binary trees where each node represents a point in k-dimensional space. The tree recursively partitions the space along alternating dimensions, creating a hierarchical structure that allows for faster searching in multidimensional data.\n",
    "\n",
    "#### Indexing\n",
    "\n",
    "1. **Tree Construction**:\n",
    "   - Begin with the complete set of points.\n",
    "   - Choose a dimension and find the median point along that dimension.\n",
    "   - Use this median point to split the space into two subspaces.\n",
    "   - Recursively apply this process to each subspace, cycling through dimensions.\n",
    "\n",
    "2. **Insertion**:\n",
    "   - Start at the root node.\n",
    "   - Compare the new point's coordinates with the current node's splitting dimension.\n",
    "   - Traverse left or right based on this comparison.\n",
    "   - Continue until reaching a leaf node, where the new point is inserted.\n",
    "\n",
    "#### Similarity Search\n",
    "\n",
    "1. **Nearest Neighbor Search**:\n",
    "   - Begin at the root and traverse the tree as if inserting the query point.\n",
    "   - Keep track of the closest point found so far.\n",
    "   - Use distance calculations to determine if other branches need to be explored.\n",
    "\n",
    "2. **Backtracking**:\n",
    "   - After reaching a leaf, backtrack up the tree.\n",
    "   - At each level, check if the other branch could contain a closer point.\n",
    "   - Explore promising branches based on distance calculations.\n",
    "\n",
    "3. **Performance Considerations**:\n",
    "   - Search efficiency decreases as dimensionality increases (curse of dimensionality).\n",
    "   - In low to moderate dimensions, KD-Trees offer logarithmic time complexity for queries.\n",
    "   - For high-dimensional data, performance may degrade to linear time in worst cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abec3d0-2324-4ca6-b11b-2fb424e242a6",
   "metadata": {},
   "source": [
    "### Locality Sensitive Hashing (LSH)\n",
    "\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F606382d0ca90a8d24f26780f5f9954123e37be91-575x603.png&w=1200&q=75\" width=600>\n",
    "\n",
    "#### Basic Concept\n",
    "\n",
    "Locality-Sensitive Hashing uses hash functions that map similar items to the same hash value with higher probability than dissimilar items. Unlike traditional hash functions that aim to minimize collisions, LSH intentionally causes collisions for similar items.\n",
    "\n",
    "#### Indexing\n",
    "\n",
    "1. **Hash Function Selection**:\n",
    "   - Choose a family of hash functions that are locality-sensitive for the distance measure in use (e.g., Euclidean distance, cosine similarity).\n",
    "   - Common LSH families include:\n",
    "     - Random Projections for cosine similarity\n",
    "     - p-stable distributions for Lp distances\n",
    "\n",
    "2. **Hash Table Construction**:\n",
    "   - Create multiple hash tables, each using a different combination of hash functions.\n",
    "   - For each vector in the dataset:\n",
    "     - Compute its hash value using the hash functions for each table.\n",
    "     - Store the vector in the corresponding bucket in each hash table.\n",
    "\n",
    "#### Similarity Search\n",
    "\n",
    "1. **Query Processing**:\n",
    "   - Hash the query vector using the same hash functions used for indexing.\n",
    "   - Retrieve vectors from the buckets that match the query's hash values.\n",
    "\n",
    "2. **Candidate Set Generation**:\n",
    "   - Combine the vectors retrieved from all hash tables into a candidate set.\n",
    "   - Remove duplicates from the candidate set.\n",
    "\n",
    "3. **Distance Computation**:\n",
    "   - Compute actual distances between the query vector and vectors in the candidate set.\n",
    "   - Return the k nearest neighbors based on these computed distances.\n",
    "\n",
    "#### Performance Considerations\n",
    "\n",
    "- **Efficiency**: LSH can achieve sublinear query time in high-dimensional spaces.\n",
    "- **Accuracy vs. Speed**: Adjusting the number of hash tables and hash functions allows trade-offs between query speed and result accuracy.\n",
    "- **Dimensionality**: LSH performs well in high-dimensional spaces where tree-based methods often struggle.\n",
    "- **Memory Usage**: Multiple hash tables can consume significant memory, especially for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bd8ef0-f55a-489d-9c6f-ba0e2e3f7a4b",
   "metadata": {},
   "source": [
    "### Product Quantization (PQ)\n",
    "\n",
    "<img src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F791910350d7d2140dbe684b405ef5ee761c8fc6a-1060x720.png&w=3840&q=75\" width=600>\n",
    "\n",
    "#### Basic Concept\n",
    "\n",
    "PQ decomposes high-dimensional vectors into multiple lower-dimensional subvectors. Each subvector is then quantized using a smaller codebook, effectively compressing the original vector. This allows for compact storage and fast distance computations.\n",
    "\n",
    "#### Indexing\n",
    "\n",
    "1. **Vector Decomposition**:\n",
    "   - Divide each d-dimensional vector into m subvectors of d/m dimensions each.\n",
    "\n",
    "2. **Codebook Generation**:\n",
    "   - For each of the m subspaces:\n",
    "     - Apply k-means clustering to the subvectors in that subspace.\n",
    "     - The resulting k cluster centroids form the codebook for that subspace.\n",
    "\n",
    "3. **Vector Encoding**:\n",
    "   - For each original vector:\n",
    "     - Replace each subvector with the index of its nearest centroid in the corresponding codebook.\n",
    "     - Store the resulting sequence of m indices (typically bytes) instead of the full vector.\n",
    "\n",
    "#### Similarity Search\n",
    "\n",
    "1. **Query Preparation**:\n",
    "   - Decompose the query vector into m subvectors.\n",
    "\n",
    "2. **Distance Table Computation**:\n",
    "   - For each subspace, compute distances between the query subvector and all centroids in the codebook.\n",
    "   - Store these distances in lookup tables.\n",
    "\n",
    "3. **Approximate Distance Calculation**:\n",
    "   - For each database vector:\n",
    "     - Sum the precomputed distances for each subvector using the lookup tables.\n",
    "     - This sum approximates the distance between the query and the database vector.\n",
    "\n",
    "4. **Candidate Selection**:\n",
    "   - Return the vectors with the smallest approximate distances as the nearest neighbors.\n",
    "\n",
    "#### Performance Considerations\n",
    "\n",
    "- **Compression**: PQ significantly reduces memory usage, allowing larger datasets to fit in memory.\n",
    "- **Search Speed**: Distance calculations are performed on compressed representations, greatly accelerating search.\n",
    "- **Accuracy vs. Compression**: Increasing m (number of subspaces) improves compression but may reduce accuracy. Increasing k (codebook size) improves accuracy but increases memory usage.\n",
    "- **Dimensionality**: PQ handles high-dimensional data well, making it suitable for modern embedding vectors.\n",
    "- **Adaptability**: Can be combined with other techniques (e.g., Inverted File Index) for further performance improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68924a1e-74db-45e9-8128-4e4f3754e3ce",
   "metadata": {},
   "source": [
    "### Inverted File Index (IVF)\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1286/1*o6tLFAPCY5kZWIGOLOn8gw.png\" width=600>\n",
    "\n",
    "#### Basic Concept\n",
    "\n",
    "IVF divides the vector space into a number of clusters, each represented by a centroid. Vectors are assigned to their nearest centroid, and an inverted index maps each centroid to its member vectors. This structure allows for efficient pruning of the search space during queries.\n",
    "\n",
    "#### Indexing\n",
    "\n",
    "1. **Cluster Generation**:\n",
    "   - Apply k-means clustering to a sample of the dataset to generate a set of centroids.\n",
    "   - The number of centroids (k) is typically much smaller than the total number of vectors.\n",
    "\n",
    "2. **Vector Assignment**:\n",
    "   - For each vector in the dataset:\n",
    "     - Find the nearest centroid.\n",
    "     - Assign the vector to this centroid's cluster.\n",
    "\n",
    "3. **Inverted Index Construction**:\n",
    "   - Create a data structure that maps each centroid to the list of vectors assigned to it.\n",
    "\n",
    "#### Similarity Search\n",
    "\n",
    "1. **Query Processing**:\n",
    "   - Find the nearest centroids to the query vector.\n",
    "   - Typically, multiple (e.g., nprobe) nearest centroids are considered.\n",
    "\n",
    "2. **Candidate Set Generation**:\n",
    "   - Retrieve the vectors assigned to the selected centroids using the inverted index.\n",
    "   - These vectors form the candidate set for detailed distance computation.\n",
    "\n",
    "3. **Refinement**:\n",
    "   - Compute actual distances between the query vector and the candidates.\n",
    "   - Return the k nearest vectors based on these computed distances.\n",
    "\n",
    "#### Performance Considerations\n",
    "\n",
    "- **Search Efficiency**: By limiting detailed distance computations to a subset of the data, IVF significantly speeds up search in large datasets.\n",
    "- **Accuracy vs. Speed Trade-off**: Adjusting the number of clusters (k) and the number of probed clusters (nprobe) allows for balancing between search speed and result accuracy.\n",
    "- **Memory Usage**: IVF typically requires less additional memory compared to some other indexing structures, as it mainly stores cluster assignments.\n",
    "- **Scalability**: Well-suited for large-scale datasets, as search complexity grows sublinearly with dataset size.\n",
    "- **Combination with Other Techniques**: Often combined with compression methods like Product Quantization (IVF-PQ) for further efficiency gains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b53f5a4-c24b-4b60-856c-4d96b4d2648a",
   "metadata": {},
   "source": [
    "### Hierarchichal Navigable Small World (HNSW)\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ziU6_KIDqfmaDXKA1cMa8w.png\" width=600>\n",
    "\n",
    "#### Basic Concept\n",
    "\n",
    "HNSW builds a multilayered graph where each layer is a \"navigable small world\" - a graph with both short-range and long-range connections. The top layer is sparsely connected, with density increasing in lower layers. This structure allows for efficient logarithmic-complexity searches.\n",
    "\n",
    "#### Indexing\n",
    "\n",
    "1. **Multilayer Graph Construction**:\n",
    "   - For each vector:\n",
    "     - Randomly assign it to a maximum layer level based on an exponential distribution.\n",
    "     - Insert the vector into all layers from its maximum level down to the bottom layer.\n",
    "\n",
    "2. **Node Insertion (per layer)**:\n",
    "   - Use the layer above (if exists) as an entry point.\n",
    "   - Perform a greedy search to find M nearest neighbors in the current layer.\n",
    "   - Connect the new node bidirectionally to these neighbors, maintaining a maximum of M connections per node.\n",
    "\n",
    "3. **Graph Optimization**:\n",
    "   - Periodically or during insertion, optimize connections to ensure a good balance of short and long-range links.\n",
    "\n",
    "#### Similarity Search\n",
    "\n",
    "1. **Entry Point Selection**:\n",
    "   - Start at the entry point of the top layer.\n",
    "\n",
    "2. **Greedy Search**:\n",
    "   - At each layer, perform a greedy search to find the nearest neighbor to the query.\n",
    "   - Use this neighbor as the entry point for the next lower layer.\n",
    "\n",
    "3. **Beam Search in Bottom Layer**:\n",
    "   - In the bottom (densest) layer, perform a beam search.\n",
    "   - Maintain a dynamic list of best candidates, expanding the search to their neighbors.\n",
    "\n",
    "4. **Result Refinement**:\n",
    "   - Return the k nearest neighbors found during the beam search.\n",
    "\n",
    "#### Performance Considerations\n",
    "\n",
    "- **Search Complexity**: Achieves logarithmic time complexity for search operations, making it extremely fast even for large datasets.\n",
    "- **Accuracy**: Provides high accuracy, often comparable to exact search methods.\n",
    "- **Memory Usage**: Requires more memory than some other ANN algorithms due to storing the graph structure.\n",
    "- **Incremental Updates**: Supports efficient insertion of new items without rebuilding the entire index.\n",
    "- **Scalability**: Excellent performance on both CPU and GPU, making it suitable for various hardware setups.\n",
    "- **Tuning**: Parameters like the number of connections per node (M) and the size of the dynamic candidate list during search can be tuned to balance between search speed and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02142144-3945-45de-9ea4-5399ac387aa8",
   "metadata": {},
   "source": [
    "---\n",
    "# Vector Database and RAG Setup\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*kPXvR_-LxPfdsojc.png\" width=600>\n",
    "\n",
    "Let's put it all together into a retrieval augmented generation flow:\n",
    "1. Chunking text\n",
    "2. Creating a Vector Database\n",
    "3. Indexxing the text chunks\n",
    "4. Setting up an LLM\n",
    "5. Connecting retrieval with LLM for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ff2cf6a-8225-4bb5-94ef-13d54a53cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Dependencies\n",
    "import os\n",
    "import chromadb\n",
    "from langchain_openai import ChatOpenAI\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c8998a-ac12-4d7b-8001-b5d8f7990d5e",
   "metadata": {},
   "source": [
    "### Chunking the Text\n",
    "\n",
    "We'll be using the [TIES-Merging: Resolving Interference When Merging Models](https://arxiv.org/abs/2306.01708#) paper for our example, along with LangChain integrations with PDF loaders and chunkers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73083a12-1c24-408d-b3b3-ca95fa8ca067",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"/Users/alucek/Documents/Jupyter_Notebooks/embeddings-guide/ties_merging.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "document = \"\"\n",
    "for i in range(len(pages)):\n",
    "    document += pages[i].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172f2ede-64f5-45d2-bf3c-616699f65937",
   "metadata": {},
   "source": [
    "We'll be using a recursive token based chunking system, following [OpenAI's File Search tool parameters](https://platform.openai.com/docs/assistants/tools/file-search/how-it-works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0b23e20-1396-4d4a-b90d-3ac92f28033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4\",\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=400,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b359f5c0-dea1-4d50-bc38-849a58c1264b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 \n",
      "\n",
      "TIES-MERGING : Resolving Interference When\n",
      "Merging Models\n",
      "Prateek Yadav1Derek Tam1\n",
      "Leshem Choshen2,3Colin Raffel1Mohit Bansal1\n",
      "1University of North Carolina at Chapel Hill2IBM Research3MIT\n",
      "leshem.choshen@ibm.com\n",
      "{praty,dtredsox,craffel,mbansal}@cs.unc.edu\n",
      "Abstract\n",
      "Transfer learning – i.e., further fine-tuning a pre-trained model on a downstream\n",
      "task – can confer significant advantages, including improved downstream perfor-\n",
      "mance, faster convergence, and better sample efficiency. These advantages\n"
     ]
    }
   ],
   "source": [
    "print(len(chunks), \"\\n\")\n",
    "print(chunks[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1624d5-b5b6-43fd-b0f7-24bd465f14b1",
   "metadata": {},
   "source": [
    "### Indexxing into ChromaDB\n",
    "\n",
    "For our vector database of choice, we'll be using [ChromaDB](https://www.trychroma.com/), an open source and lightweight vector database. By default they index vectors using Hierarchical Navigable Small World."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b400c2d9-d974-42f2-b006-c9c5edf89ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/alucek/Documents/Jupyter_Notebooks/embeddings-guide/chromadb\"\n",
    "client = chromadb.PersistentClient(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23cf08f8-f87d-4078-8824-93d5075a3226",
   "metadata": {},
   "outputs": [],
   "source": [
    "ties_collection = client.get_or_create_collection(name='ties_collection', metadata={\"hnsw:space\": \"cosine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f3e7c5c-df30-4ac2-864d-326e7d68bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for chunk in chunks:\n",
    "    ties_collection.add(\n",
    "    documents=[chunk],\n",
    "    ids=[f\"chunk_{i}\"]\n",
    "    )\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "862ddbdd-aca1-4044-ac4c-1dc553a0ec7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ties_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86f72c5f-2752-4538-b32d-701715e62e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the process for electing signs?\"\n",
    "\n",
    "results = ties_collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92e9dfed-c433-4cb6-808b-a33388eec828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "\n",
      "Retrieved Doc:  1\n",
      "Chunk:  chunk_22\n",
      "Distance:  0.6302816389013587\n",
      "Text Snippet:  (see#in Fig. 2). In the Appendix B.5, we bring a more detailed view, including a comparison to\n",
      "applying TIES-MERGING and show reducing interference encourages diversity in specific parameter\n",
      "values (std) together with the similarity of their influence (mean).\n",
      "(b) Importance of Resolving Sign Interference. To quantify the impact of sign interference,\n",
      "we group the parameters by their sign agreement . A value of 0.5indicates an equal number of\n",
      "positive and negative signs for a given parameter acros...\n",
      "-------\n",
      "\n",
      "Retrieved Doc:  2\n",
      "Chunk:  chunk_10\n",
      "Distance:  0.7073322213708346\n",
      "Text Snippet:  sign vector γt∈Rdand a magnitude vector µt∈Rdasτt=γt⊙µt, where ⊙is the elementwise\n",
      "product. Formally, γt=sgn(τt), where sgn(x)∗ |x|=xand returns a value of +1,0, or−1. The\n",
      "magnitude vector µtis defined as µt=|τt|and the value µi\n",
      "ttells us the movement required in the\n",
      "i-th dimension from the initialization.\n",
      "4.2 Steps in T IES-MERGING\n",
      "To merge multiple task-specific models {θt}n\n",
      "t=1, we first create corresponding task vectors {τt}n\n",
      "t=1.\n",
      "Given these task vectors, TIES-MERGING method follows three s...\n",
      "-------\n",
      "\n",
      "Retrieved Doc:  3\n",
      "Chunk:  chunk_43\n",
      "Distance:  0.7408111240524549\n",
      "Text Snippet:  individual-task models to create a multitask still lags behind the simultaneous multitask training.\n",
      "Moreover, it is not clear how to select the checkpoints for merging in order to create multitask models\n",
      "useful for specific domains. In addition, while our method provides a way to choose signs when\n",
      "merging task vectors, we still find that using the signs from a multitask model performs better. Some\n",
      "potential future works include figuring out a good way to estimate multitask signs without having\n",
      "a...\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"-------\\n\")\n",
    "    print(\"Retrieved Doc: \", i+1)\n",
    "    print(\"Chunk: \", results['ids'][0][i])\n",
    "    print(\"Distance: \", results['distances'][0][i])\n",
    "    print(\"Text Snippet: \", f\"{results['documents'][0][i][:500]}...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e82bb2-7aef-4295-9423-a82ab7d7e538",
   "metadata": {},
   "source": [
    "### LLM & Prompt Setup\n",
    "\n",
    "Now that we have the vector database setup, the second half is setting up our LLM to be able to take in the retrieved documents and the user query to provide a retrieval augemented generation response.\n",
    "\n",
    "We'll be using LangChain with GPT-4o to set this dynamic prompt up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0dccb59-7802-4fd0-a8a3-c2390300338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt_template = \"\"\"\n",
    "You are a data science AI assistant. \n",
    "Respond to the user question below using the provided contextual documents retrieved from a research paper.\n",
    "\n",
    "Context Documents: {documents}\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_prompt_template)\n",
    "llm = ChatOpenAI(temperature=0.0, model=\"gpt-4o\")\n",
    "\n",
    "rag_chain = rag_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff257a6-64cd-42e7-b72f-5acaaf1df4ed",
   "metadata": {},
   "source": [
    "### Full RAG Pipeline\n",
    "\n",
    "Now we put it together to take the user's question, retrieve relevant documents from the vector database with it, and pass the question + context to the LLM to generate a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a13db389-916e-4905-b5cb-b782d9a6c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(query):\n",
    "    results = ties_collection.query(query_texts=[query], n_results=5)\n",
    "    documents = results['documents'][0]\n",
    "    response = rag_chain.invoke({\"question\": query, \"documents\": documents})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e0464a1-3546-4df9-b275-0e62774b8f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In the TIES-MERGING process, the sign election step is crucial for resolving disagreements in the sign of each parameter across different models. Here’s a detailed explanation of how sign election happens:\n",
       "\n",
       "1. **Aggregate Elected Sign Vector Creation**:\n",
       "   - For each parameter \\( p \\) in the task vectors, the method first separates the values based on their sign (positive or negative).\n",
       "   - The total magnitude (or mass) of the positive and negative values is then calculated. This is done by summing the magnitudes of the values that have positive signs and those that have negative signs separately.\n",
       "\n",
       "2. **Choosing the Dominant Sign**:\n",
       "   - The sign with the highest total magnitude is chosen as the elected sign for that parameter. This means if the sum of the magnitudes of the positive values is greater than the sum of the magnitudes of the negative values, the elected sign will be positive, and vice versa.\n",
       "\n",
       "3. **Efficient Computation**:\n",
       "   - This process can be efficiently computed using the formula:\n",
       "     \\[\n",
       "     \\gamma_p^m = \\text{sgn}\\left(\\sum_{t=1}^{n} \\hat{\\tau}_p^t\\right)\n",
       "     \\]\n",
       "     where \\( \\gamma_p^m \\) is the elected sign for parameter \\( p \\), and \\( \\hat{\\tau}_p^t \\) represents the value of parameter \\( p \\) in the task vector \\( \\hat{\\tau}_t \\) for model \\( t \\).\n",
       "\n",
       "By following these steps, the TIES-MERGING method ensures that the elected sign for each parameter is the one that has the greatest overall influence across all the models being merged. This helps in reducing sign interference and maintaining the integrity of the parameter values during the merging process."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = rag_query(\"How does sign election happen at that step of the process?\")\n",
    "\n",
    "display(Markdown(output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c488fc01-5ea0-435f-bdb7-97af0009c8c3",
   "metadata": {},
   "source": [
    "---\n",
    "# Other Use Cases for Embeddings\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*svydqyHkUxl3tbP5eADecg.png\" width=450>\n",
    "\n",
    "1. **Clustering:**\n",
    "Embeddings can be used to group similar documents, sentences, or words together in high-dimensional space. This is particularly useful for organizing large collections of text data, such as categorizing news articles or grouping customer feedback. Clustering can reveal hidden patterns or themes within the data that may not be immediately apparent.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1078/1*byMr5hdnA4S5WCsQodZ39Q.png\" width=450>\n",
    "\n",
    "2. **Recommendations:**\n",
    "By comparing the embeddings of items a user has interacted with to other items in a database, recommendation systems can suggest similar content. This technique is widely used in e-commerce to recommend products, in content platforms to suggest articles or videos, and in music streaming services to create personalized playlists.\n",
    "\n",
    "<img src=\"https://pub.mdpi-res.com/electronics/electronics-13-02625/article_deploy/html/images/electronics-13-02625-g001.png?1722331959\" width=450>\n",
    "\n",
    "3. **Anomaly detection:**\n",
    "Embeddings can help identify unusual or anomalous data points that don't fit well with the rest of the dataset. This is valuable in various fields, such as fraud detection in financial transactions, identifying unusual network traffic for cybersecurity, or detecting manufacturing defects in quality control processes.\n",
    "\n",
    "<img src=\"https://dl.acm.org/cms/attachment/html/10.1145/3411764.3445782/assets/html/images/image38.png\" width=450>\n",
    "\n",
    "4. **Diversity measurement:**\n",
    "By examining the distribution of embeddings in a dataset, one can assess the diversity of the content. This can be useful in content curation to ensure a wide range of perspectives, in team formation to balance skill sets, or in academic research to evaluate the breadth of a literature review.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*XjDmui7Zprb809NsMNBV4Q.png\" width=450>\n",
    "\n",
    "5. **Classification:**\n",
    "Embeddings can be used to classify content into predefined categories by comparing the embedding of a new piece of content to the embeddings of known categories. This approach is often more flexible than traditional classification methods, especially when dealing with new or ambiguous examples, and can be particularly effective for multi-label classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebf53ef-9f0f-4f80-9160-be2c941d30ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
